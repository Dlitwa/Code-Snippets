{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f4ba51",
   "metadata": {},
   "source": [
    "# Pull in dropped ID back to y_pred , y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd015a",
   "metadata": {},
   "source": [
    "id = df['id']\n",
    "\n",
    "y=df['target']\n",
    "X=df.drop('target', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size =0.8)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = y_test.to_frame('Actual Value')\n",
    "results.insert(0, 'Predicted Value'), y_pred)\n",
    "results = id.to_frame().join(results, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58764595",
   "metadata": {},
   "source": [
    "# Drop Columns Function for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def drop_cols(df,cols_to_drop):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    ret_df = df_copy.drop(columns=cols_to_drop)\n",
    "    return ret_df\n",
    "\n",
    "cols_to_drop = ['trans_date_trans_time', 'merchant', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', \n",
    "               'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'hour', 'weekday', 'region', 'age']\n",
    "drop_cols_trans = FunctionTransformer(drop_cols, kw_args={'cols_to_drop':cols_to_drop})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82647f67",
   "metadata": {},
   "source": [
    "# chi square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "crosstab = pd.crosstab(df['gender'], df['is_fraud'])\n",
    "print(crosstab)\n",
    "stats.chi2_contingency(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d3d40",
   "metadata": {},
   "source": [
    "# correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa85d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "df['dob'] = pd.to_datetime(df['dob'], format='%Y-%m-%d')\n",
    "df['age'] = (date.today() - df['dob']) // timedelta(days=365.2425)\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d3e4f",
   "metadata": {},
   "source": [
    "# describe for categorical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app_record.describe(include = 'object'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859fe8e",
   "metadata": {},
   "source": [
    "# get feature names from preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "\n",
    "get_feature_names(preprocessor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
